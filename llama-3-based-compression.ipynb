{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96abd349-a98f-4d7f-ba96-45b7c2f3e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57a78274-8f7a-4959-801e-c26d7249d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"\"\"\n",
    "The University of Washington (UW and informally U-Dub or U Dub) is a public research university in Seattle, Washington, United States. Founded in 1861, the University of Washington is one of the oldest universities on the West Coast of the United States.\n",
    "\n",
    "The university has a 703-acre (284 ha) main campus located in the city's University District. It also has satellite campuses in nearby cities of Tacoma and Bothell. Overall, UW encompasses more than 500 buildings and over 20 million gross square footage of space, including one of the largest library systems in the world with more than 26 university libraries, art centers, museums, laboratories, lecture halls, and stadiums.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "459280d4-b3c5-4aa4-9f3b-789697edcf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d948e0389258417e8bc0e415b32f0425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b9ad2bc2-293a-4548-9e83-1c628f17b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1b18d226-8aec-4d69-a02b-e69b1bf63a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,    198,    791,   3907,    315,   6652,    320,     52,     54,\n",
       "            323,   6179,    750,    549,   9607,    392,    477,    549,  17533,\n",
       "              8,    374,    264,    586,   3495,  12374,    304,  16759,     11,\n",
       "           6652,     11,   3723,   4273,     13,  78811,    304,    220,   9714,\n",
       "             16,     11,    279,   3907,    315,   6652,    374,    832,    315,\n",
       "            279,  24417,  23978,    389,    279,   4410,  16377,    315,    279,\n",
       "           3723,   4273,    382,    791,  12374,    706,    264,    220,  20436,\n",
       "          64434,    320,  17058,   6520,      8,   1925,  15679,   7559,    304,\n",
       "            279,   3363,    596,   3907,  11182,     13,   1102,   1101,    706,\n",
       "          24088,  53008,    304,  14373,   9919,    315,  85628,    323,  11995,\n",
       "            616,     13,  28993,     11,  66716,  71010,    810,   1109,    220,\n",
       "           2636,  14016,    323,    927,    220,    508,   3610,  20547,   9518,\n",
       "          22609,    315,   3634,     11,   2737,    832,    315,    279,   7928,\n",
       "           6875,   6067,    304,    279,   1917,    449,    810,   1109,    220,\n",
       "           1627,  12374,  20797,     11,   1989,  19169,     11,  51677,     11,\n",
       "          70760,     11,  31678,  52473,     11,    323,  90184,    627]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a12d6e52-40fe-47a0-9d35-8cde6f0a0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids)\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4a50a67f-e487-4fd3-9ae1-29cf247f63a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.8699,  8.7921, 12.9562,  ..., -4.4354, -4.4354, -4.4355],\n",
       "         [ 8.9626,  7.1396, 12.8710,  ..., -6.3913, -6.3915, -6.3915],\n",
       "         [-1.3854, -0.5908, -1.5957,  ..., -9.4544, -9.4544, -9.4544],\n",
       "         ...,\n",
       "         [ 2.6486,  2.1777,  0.4392,  ..., -3.0379, -3.0381, -3.0380],\n",
       "         [ 8.4127,  5.4962,  8.9823,  ..., -2.1911, -2.1912, -2.1911],\n",
       "         [ 7.6229,  5.7835,  7.6811,  ..., -6.9104, -6.9103, -6.9103]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "46435a73-cd5b-46bc-8436-330522bf80ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = torch.argsort(logits, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "03593c9d-5ac7-4058-b803-b3ddedd82f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 14924,    755,      2,  ..., 103273,  51202,    350],\n",
       "         [   475,      2,   1527,  ..., 113640, 118554,  82000],\n",
       "         [ 17200,  31613,   3907,  ...,  97865,  13027,  31488],\n",
       "         ...,\n",
       "         [ 10034,  29703,  74175,  ...,  60285, 104880,   3482],\n",
       "         [    13,    382,     11,  ...,  62758, 117298, 126503],\n",
       "         [128001,    791,     52,  ...,  48046,  13765,  96693]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "69342a14-8730-4548-ad0f-089eeda1dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rank = (input_ids.unsqueeze(-1).squeeze(0)[1:] == sorted_indices.squeeze(0)[:-1]).nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ecb285b2-1d41-4470-8e9d-da24d1734964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49, 56,  2,  0,  3,  6,  0,  0, 24, 20,  0,  1,  2,  0,  1,  0,  2,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  1,  2,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,\n",
       "         0,  3,  0,  2,  0,  4,  4,  0, 10,  0,  1,  1,  0,  0,  1,  1,  0,  0,\n",
       "         1,  9,  0,  0,  1,  1,  1,  0,  2,  0,  0, 19,  2,  3,  1,  0,  0,  0,\n",
       "         1, 36,  0,  1,  0,  2,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,\n",
       "         0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         1,  1,  0, 48,  5,  0,  0,  0,  3,  0, 16,  0,  0,  0,  4,  4],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank # rank is the compressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3744adc5-4114-459f-b3c8-ed29622559d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start decompression\n",
    "seq_len = input_ids.shape[1]\n",
    "\n",
    "decompressed_outputs = torch.zeros_like(input_ids)\n",
    "decompressed_outputs[0, 0] = 128000 # 128000 is the special token index indicating the start of sequence\n",
    "for step in range(1, seq_len):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=decompressed_outputs[..., :step])\n",
    "    logits = outputs.logits[0, step - 1]\n",
    "    sorted_indices = torch.argsort(logits, descending=True)\n",
    "    decompressed_token = sorted_indices[rank[step - 1]]\n",
    "    decompressed_outputs[0, step] = decompressed_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "df7c11b3-97d1-4b39-9a4c-2137f6be3ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,    198,    791,   3907,    315,   6652,    320,     52,     54,\n",
       "            323,   6179,    750,    549,   9607,    392,    477,    549,  17533,\n",
       "              8,    374,    264,    586,   3495,  12374,    304,  16759,     11,\n",
       "           6652,     11,   3723,   4273,     13,  78811,    304,    220,   9714,\n",
       "             16,     11,    279,   3907,    315,   6652,    374,    832,    315,\n",
       "            279,  24417,  23978,    389,    279,   4410,  16377,    315,    279,\n",
       "           3723,   4273,    382,    791,  12374,    706,    264,    220,  20436,\n",
       "          64434,    320,  17058,   6520,      8,   1925,  15679,   7559,    304,\n",
       "            279,   3363,    596,   3907,  11182,     13,   1102,   1101,    706,\n",
       "          24088,  53008,    304,  14373,   9919,    315,  85628,    323,  11995,\n",
       "            616,     13,  28993,     11,  66716,  71010,    810,   1109,    220,\n",
       "           2636,  14016,    323,    927,    220,    508,   3610,  20547,   9518,\n",
       "          22609,    315,   3634,     11,   2737,    832,    315,    279,   7928,\n",
       "           6875,   6067,    304,    279,   1917,    449,    810,   1109,    220,\n",
       "           1627,  12374,  20797,     11,   1989,  19169,     11,  51677,     11,\n",
       "          70760,     11,  31678,  52473,     11,    323,  90184,    627]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decompressed_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b2995c84-6393-4a31-8f1b-8fe8762011b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(\n",
    "    decompressed_outputs[0],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    "    spaces_between_special_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7961426c-0114-4342-b43e-eb669abfacd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe University of Washington (UW and informally U-Dub or U Dub) is a public research university in Seattle, Washington, United States. Founded in 1861, the University of Washington is one of the oldest universities on the West Coast of the United States.\\n\\nThe university has a 703-acre (284 ha) main campus located in the city's University District. It also has satellite campuses in nearby cities of Tacoma and Bothell. Overall, UW encompasses more than 500 buildings and over 20 million gross square footage of space, including one of the largest library systems in the world with more than 26 university libraries, art centers, museums, laboratories, lecture halls, and stadiums.\\n\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67c59f-869e-43d3-b069-f88f2395ec6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
